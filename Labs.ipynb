{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering (Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data from CSV file\n",
    "data = pd.read_csv(\"Mall_Customers.csv\")\n",
    "\n",
    "# Extract the required columns\n",
    "X = data[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Determine the number of clusters (you can modify this)\n",
    "k = 5\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(X)\n",
    "kmeans_labels = kmeans.labels_\n",
    "kmeans_centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Perform Agglomerative clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=k)\n",
    "agglomerative.fit(X)\n",
    "agglomerative_labels = agglomerative.labels_\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(X['Annual Income (k$)'], X['Spending Score (1-100)'],\n",
    "            c=kmeans_labels, cmap='viridis')\n",
    "plt.scatter(kmeans_centroids[:, 0],\n",
    "            kmeans_centroids[:, 1], marker='X', color='red')\n",
    "plt.title('K-means Clustering')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X['Annual Income (k$)'], X['Spending Score (1-100)'],\n",
    "            c=agglomerative_labels, cmap='viridis')\n",
    "plt.title('Agglomerative Clustering')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering (No Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1.1, 1.1], [1.5, 2.1], [3.1, 4.1], [\n",
    "             5.1, 7.1], [3.5, 5.1], [4.5, 5.1], [3.5, 4.5]])\n",
    "\n",
    "k = 2\n",
    "\n",
    "centroids = X[:k]\n",
    "\n",
    "labels = np.zeros(len(X))\n",
    "distances = np.zeros((len(X), k))\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "    for j in range(k):\n",
    "        distances[:, j] = np.linalg.norm(X - centroids[j], axis=1)\n",
    "\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    for j in range(k):\n",
    "        centroids[j] = np.mean(X[labels == j], axis=0)\n",
    "\n",
    "    if np.all(labels == np.argmin(distances, axis=1)):\n",
    "        break\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Centroids:\", centroids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering (No Library) - 3 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [2.5, 4.5, 5.4],\n",
    "    [3.8, 6.3, 9.8],\n",
    "    [9.5, 8.6, 6.8],\n",
    "    [4.7, 8.8, 4.2],\n",
    "    [5.5, 3.1, 9.9],\n",
    "    [2.1, 1.8, 7.8]\n",
    "]\n",
    "\n",
    "# Initial centroids (choosing the first two points from the dataset)\n",
    "centroids = [[2.5, 4.5, 5.4], [3.8, 6.3, 9.8]]\n",
    "\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    return sum((a - b) ** 2 for a, b in zip(point1, point2)) ** 0.5\n",
    "\n",
    "\n",
    "def calculate_mean(points):\n",
    "    return [\n",
    "        sum(point[i] for point in points) / len(points)\n",
    "        for i in range(len(points[0]))\n",
    "    ]\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    clusters = [[], []]\n",
    "    for point in data:\n",
    "        distances = [calculate_distance(point, centroid)\n",
    "                     for centroid in centroids]\n",
    "        closest_centroid = distances.index(min(distances))\n",
    "        clusters[closest_centroid].append(point)\n",
    "    centroids = [calculate_mean(cluster) for cluster in clusters]\n",
    "\n",
    "\n",
    "print(\"Final Centroids:\", centroids)\n",
    "\n",
    "for i in range(len(centroids)):\n",
    "    cluster_points = clusters[i]\n",
    "    cluster_centroid = centroids[i]\n",
    "    print(f\"Cluster {i+1} points: {cluster_points}\")\n",
    "    print(f\"Cluster {i+1} centroid: {cluster_centroid}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering (With Library) - 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Given dataset\n",
    "data = [\n",
    "    [2.5, 4.5, 5.4],\n",
    "    [3.8, 6.3, 9.8],\n",
    "    [9.5, 8.6, 6.8],\n",
    "    [4.7, 8.8, 4.2],\n",
    "    [5.5, 3.1, 9.9],\n",
    "    [2.1, 1.8, 7.8]\n",
    "]\n",
    "\n",
    "# Create a KMeans instance with 2 clusters\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "cluster_assignments = kmeans.labels_\n",
    "\n",
    "# Get the coordinates of the final centroids\n",
    "final_centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Output the results\n",
    "print(\"Final Centroids:\", final_centroids)\n",
    "\n",
    "for i in range(2):\n",
    "    points_in_cluster = [point for point, cluster in zip(\n",
    "        data, cluster_assignments) if cluster == i]\n",
    "    print(f\"Cluster {i+1} points: {points_in_cluster}\")\n",
    "    print(f\"Cluster {i+1} centroid: {final_centroids[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (No Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randint(10, 50, 60).reshape(10, 6)\n",
    "print(\"Original Data: \\n\", X)\n",
    "\n",
    "# Standardizing the features\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "print(\"\\n Standardized Data: \\n\", X)\n",
    "\n",
    "# Computing the covariance matrix\n",
    "covariance_matrix = np.cov(X.T)\n",
    "print(\"\\n Covariance Matrix: \\n\", covariance_matrix)\n",
    "\n",
    "\n",
    "# Computing the eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "print(\"\\n Eigenvectors: \\n\", eigenvectors)\n",
    "print(\"\\n Eigenvalues: \\n\", eigenvalues)\n",
    "\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i])\n",
    "              for i in range(len(eigenvalues))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigenpairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"\\n Eigenpairs sorted by decreasing eigenvalues: \\n\", eigenpairs)\n",
    "\n",
    "\n",
    "# We're choosing the top 2 eigenvectors\n",
    "matrix_w = np.hstack((eigenpairs[0][1].reshape(6, 1),\n",
    "                      eigenpairs[1][1].reshape(6, 1)))\n",
    "\n",
    "print('\\n Matrix W:\\n', matrix_w)\n",
    "\n",
    "\n",
    "# Transforming the original dataset\n",
    "X_pca = X.dot(matrix_w)\n",
    "print(\"\\n Transformed Data: \\n\", X_pca)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('onlineretail.csv', encoding='ISO-8859-1')\n",
    "sns.boxplot(x=df['Quantity'])\n",
    "plt.title(\"Qammar Mehmood 093\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Define a function to remove outliers using IQR(Inter Quantile Range)\n",
    "\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return df[~((df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR)))]\n",
    "\n",
    "\n",
    "# Remove outliers from 'Quantity'\n",
    "df = remove_outliers(df, 'Quantity')\n",
    "\n",
    "# Plotting the boxplot after removing outliers\n",
    "sns.boxplot(x=df['Quantity'])\n",
    "plt.title(\"Qammar Mehmood 093\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (OR Gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step function\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# Perceptron function\n",
    "\n",
    "\n",
    "def perceptron(inputs, weights, bias):\n",
    "    weighted_sum = np.dot(inputs, weights) + bias\n",
    "    return step_function(weighted_sum)\n",
    "\n",
    "\n",
    "# OR gate training data\n",
    "training_data = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.array([0.1, 0.1])\n",
    "bias = 0.2\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train the perceptron\n",
    "for _ in range(100):\n",
    "    for data in training_data:\n",
    "        inputs = data[:2]\n",
    "        target_output = data[2]\n",
    "\n",
    "        # Compute the output\n",
    "        output = perceptron(inputs, weights, bias)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        error = target_output - output\n",
    "        weights += learning_rate * error * inputs\n",
    "        bias += learning_rate * error\n",
    "\n",
    "# Test the perceptron\n",
    "test_data = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "for data in test_data:\n",
    "    inputs = data\n",
    "    output = perceptron(inputs, weights, bias)\n",
    "    print(f\"Input: {inputs}, Output: {output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (AND Gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step function\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# Perceptron function\n",
    "\n",
    "\n",
    "def perceptron(inputs, weights, bias):\n",
    "    weighted_sum = np.dot(inputs, weights) + bias\n",
    "    return step_function(weighted_sum)\n",
    "\n",
    "\n",
    "# AND gate training data\n",
    "training_data = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.array([0.1, 0.1])\n",
    "bias = 0.2\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train the perceptron\n",
    "for _ in range(100):\n",
    "    for data in training_data:\n",
    "        inputs = data[:2]\n",
    "        target_output = data[2]\n",
    "\n",
    "        # Compute the output\n",
    "        output = perceptron(inputs, weights, bias)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        error = target_output - output\n",
    "        weights += learning_rate * error * inputs\n",
    "        bias += learning_rate * error\n",
    "\n",
    "# Test the perceptron\n",
    "test_data = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "for data in test_data:\n",
    "    inputs = data\n",
    "    output = perceptron(inputs, weights, bias)\n",
    "    print(f\"Input: {inputs}, Output: {output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "\n",
    "# Split into features and target\n",
    "X = data[:, 0:8]\n",
    "y = data[:, 8]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform data preprocessing: feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "\n",
    "# Visualize the model architecture\n",
    "plot_model(model, show_shapes=True, show_layer_names=True,\n",
    "           to_file='model_architecture.png')\n",
    "\n",
    "# Display the model architecture in the notebook\n",
    "model_img = plt.imread('model_architecture.png')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(model_img)\n",
    "plt.title(\"Qammar Mehmood 01-134202-093\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.5f}')\n",
    "print(f'Test Accuracy: {test_acc:.5f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (Multiple Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('car_data.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encoding categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "for i in range(X.shape[1]):\n",
    "    X[:, i] = label_encoder.fit_transform(X[:, i])\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Visualize the model architecture\n",
    "plot_model(model, show_shapes=True, show_layer_names=True,\n",
    "           to_file='model_architecture.png')\n",
    "\n",
    "# Display the model architecture in the notebook\n",
    "model_img = plt.imread('model_architecture.png')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(model_img)\n",
    "plt.title(\"Qammar Mehmood 01-134202-093\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.5f}')\n",
    "print(f'Test Accuracy: {test_acc:.5f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (No Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = [\n",
    "    ['Small', 'Green', 'Irregular', 'No'],\n",
    "    ['Large', 'Red', 'Irregular', 'Yes'],\n",
    "    ['Large', 'Red', 'Circle', 'Yes'],\n",
    "    ['Large', 'Green', 'Circle', 'No'],\n",
    "    ['Large', 'Green', 'Irregular', 'No'],\n",
    "    ['Small', 'Red', 'Circle', 'Yes'],\n",
    "    ['Large', 'Green', 'Irregular', 'No'],\n",
    "    ['Small', 'Red', 'Irregular', 'No'],\n",
    "    ['Small', 'Green', 'Circle', 'No'],\n",
    "    ['Large', 'Red', 'Circle', 'Yes']\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "features = np.array([row[:-1] for row in data])\n",
    "labels = np.array([row[-1] for row in data])\n",
    "\n",
    "# Define function to calculate Hamming distance\n",
    "\n",
    "\n",
    "def hamming_distance(instance1, instance2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(instance1, instance2))\n",
    "\n",
    "# Define function to classify new instance using KNN\n",
    "\n",
    "\n",
    "def knn_classify(k, train_features, train_labels, test_instance):\n",
    "    distances = []\n",
    "    # Calculate Hamming distance between test instance and each training instance\n",
    "    for i in range(len(train_features)):\n",
    "        dist = hamming_distance(train_features[i], test_instance)\n",
    "        distances.append((dist, train_labels[i]))\n",
    "    # Sort distances in ascending order\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    # Select k nearest neighbors\n",
    "    k_nearest = distances[:k]\n",
    "    labels = [neighbor[1] for neighbor in k_nearest]\n",
    "    return max(set(labels), key=labels.count)\n",
    "\n",
    "\n",
    "# Test the KNN classifier\n",
    "k = 3\n",
    "test_instance = np.array(['Small', 'Red', 'Circle'])\n",
    "predicted_label = knn_classify(k, features, labels, test_instance)\n",
    "print(\"Predicted class for the test instance:\",\n",
    "      test_instance, \" : \", predicted_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier   # using sklearn library\n",
    "from sklearn.preprocessing import OrdinalEncoder     # using sklearn library\n",
    "\n",
    "# Define the dataset\n",
    "data = [\n",
    "    ['<=30', 'High', 'No', 'Fair', 'No'],\n",
    "    ['<=30', 'High', 'No', 'Excellent', 'No'],\n",
    "    ['31-40', 'High', 'No', 'Fair', 'Yes'],\n",
    "    ['>40', 'Medium', 'No', 'Fair', 'Yes'],\n",
    "    ['>40', 'Low', 'Yes', 'Fair', 'Yes'],\n",
    "    ['>40', 'Low', 'Yes', 'Excellent', 'No'],\n",
    "    ['31-40', 'Low', 'Yes', 'Excellent', 'Yes'],\n",
    "    ['<=30', 'Medium', 'No', 'Fair', 'No'],\n",
    "    ['<=30', 'Low', 'Yes', 'Fair', 'Yes'],\n",
    "    ['>40', 'Medium', 'Yes', 'Fair', 'Yes'],\n",
    "    ['<=30', 'Medium', 'Yes', 'Excellent', 'Yes'],\n",
    "    ['31-40', 'Medium', 'No', 'Excellent', 'Yes'],\n",
    "    ['31-40', 'High', 'Yes', 'Fair', 'Yes'],\n",
    "    ['>40', 'Medium', 'No', 'Excellent', 'No']\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "features = [row[:-1] for row in data]\n",
    "labels = [row[-1] for row in data]\n",
    "\n",
    "# Encode categorical features using OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "features_encoded = encoder.fit_transform(features)\n",
    "\n",
    "# Define the KNN classifier\n",
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(features_encoded, labels)\n",
    "\n",
    "# Define the test instance\n",
    "test_instance = [['<=30', 'Medium', 'Yes', 'Fair']]\n",
    "\n",
    "# Encode the test instance using the fitted encoder\n",
    "test_instance_encoded = encoder.transform(test_instance)\n",
    "\n",
    "# Predict the class label for the test instance\n",
    "predicted_label = knn.predict(test_instance_encoded)\n",
    "\n",
    "print(\"Predicted class for the test instance:\",\n",
    "      test_instance, \" : \", predicted_label[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create the dataset\n",
    "dataSetStudent = {\n",
    "    \"age\": [\"<=30\", \"<=30\", \"30...40\", \">40\", \">40\", \">40\", \"31...40\", \"<=30\", \"<=30\", \">40\", \"<=30\", \"31...40\", \"31...40\", \">40\"],\n",
    "\n",
    "    \"income\": [\"high\", \"high\", \"high\", \"medium\", \"low\", \"low\", \"low\", \"medium\", \"low\", \"medium\", \"medium\", \"medium\", \"high\", \"medium\"],\n",
    "\n",
    "    \"student\": [\"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\"],\n",
    "\n",
    "    \"credit_rating\": [\"fair\", \"excellent\", \"fair\", \"fair\", \"fair\", \"excellent\", \"excellent\", \"fair\", \"fair\", \"fair\", \"excellent\", \"excellent\", \"fair\", \"excellent\"],\n",
    "\n",
    "    \"buys_computer\": [\"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"]\n",
    "}\n",
    "# print data in tabular form\n",
    "print(\"Data:\")\n",
    "print(pd.DataFrame(dataSetStudent, columns=[\n",
    "      \"age\", \"income\", \"student\", \"credit_rating\", \"buys_computer\"], index=range(1, 15), dtype=\"category\", copy=True))\n",
    "\n",
    "dataFrame = pd.DataFrame(dataSetStudent)\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "dataFrame = pd.get_dummies(dataFrame)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "features = dataFrame.drop([\"buys_computer_no\", \"buys_computer_yes\"], axis=1)\n",
    "y = dataFrame[\"buys_computer_no\"]\n",
    "\n",
    "# Create an instance of the Naïve Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(features, y)\n",
    "\n",
    "# Define the test instance\n",
    "test_instance = {\n",
    "    \"age_<=30\": 1,\n",
    "    \"age_30...40\": 0,\n",
    "    \"age_>40\": 0,\n",
    "    \"income_high\": 0,\n",
    "    \"income_low\": 0,\n",
    "    \"income_medium\": 1,\n",
    "    \"student_no\": 0,\n",
    "    \"student_yes\": 1,\n",
    "    \"credit_rating_excellent\": 0,\n",
    "    \"credit_rating_fair\": 1\n",
    "}\n",
    "\n",
    "# Convert the test instance to a DataFrame\n",
    "test_dataframe = pd.DataFrame([test_instance])\n",
    "\n",
    "features_with_value_1 = test_dataframe.columns[test_dataframe.iloc[0] == 1].tolist(\n",
    ")\n",
    "\n",
    "print(\"\\nTest Instances : \")\n",
    "for feature in features_with_value_1:\n",
    "    print(\"\\t\\t\", f\"{feature} = 1\")\n",
    "\n",
    "\n",
    "# Add missing feature columns in test_df with value 0\n",
    "missing_features = set(features.columns) - set(test_dataframe.columns)\n",
    "for feature in missing_features:\n",
    "    test_dataframe[feature] = 0\n",
    "\n",
    "# Reorder columns to match the training data\n",
    "test_dataframe = test_dataframe[features.columns]\n",
    "\n",
    "# Make predictions for the test instance\n",
    "predictions = model.predict(test_dataframe)\n",
    "\n",
    "# Convert the predictions back to original labels\n",
    "predicted_labels = [\"no\" if pred == 1 else \"yes\" for pred in predictions]\n",
    "\n",
    "print(\"\\nPridiction = \", predicted_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Random Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create the dataset\n",
    "dataSet = {\n",
    "    \"Day\": [\"D1\", \"D2\", \"D3\", \"D4\", \"D5\", \"D6\", \"D7\", \"D8\", \"D9\", \"D10\", \"D11\", \"D12\", \"D13\", \"D14\"],\n",
    "    \"Outlook\": [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\"],\n",
    "    \"Temperature\": [\"Hot\", \"Hot\", \"Hot\", \"Mild\", \"Cool\", \"Cool\", \"Cool\", \"Mild\", \"Cool\", \"Mild\", \"Mild\", \"Mild\", \"Hot\", \"Mild\"],\n",
    "    \"Humidity\": [\"High\", \"High\", \"High\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"High\"],\n",
    "    \"Wind\": [\"Weak\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Strong\"],\n",
    "    \"Play Tennis\": [\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "}\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "dataFrame = pd.DataFrame(dataSet)\n",
    "print(\"Data:\")\n",
    "print(pd.DataFrame(dataSet, columns=[\"Day\", \"Outlook\", \"Temperature\",\n",
    "      \"Humidity\", \"Wind\", \"Play Tennis\"], index=range(1, 15), dtype=\"category\"))\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "dataFrame = pd.get_dummies(dataFrame)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = dataFrame.drop([\"Play Tennis_No\", \"Play Tennis_Yes\"], axis=1)\n",
    "y = dataFrame[\"Play Tennis_No\"]\n",
    "\n",
    "# Split the data into train and test sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Create an instance of the Naïve Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predictions back to original labels\n",
    "predicted_labels = [\"No\" if pred == 1 else \"Yes\" for pred in y_pred]\n",
    "\n",
    "\n",
    "# Define the test instance\n",
    "test_instance = {\n",
    "    \"Outlook_Sunny\": 0,\n",
    "    \"Outlook_Overcast\": 0,\n",
    "    \"Outlook_Rain\": 1,\n",
    "    \"Temperature_Hot\": 0,\n",
    "    \"Temperature_Mild\": 1,\n",
    "    \"Temperature_Cool\": 0,\n",
    "    \"Humidity_High\": 1,\n",
    "    \"Humidity_Normal\": 0,\n",
    "    \"Wind_Weak\": 0,\n",
    "    \"Wind_Strong\": 1\n",
    "}\n",
    "\n",
    "# Convert the test instance to a DataFrame\n",
    "test_dataframe = pd.DataFrame([test_instance])\n",
    "\n",
    "# Add missing feature columns in test_dataframe with value 0\n",
    "missing_features = set(X_train.columns) - set(test_dataframe.columns)\n",
    "for feature in missing_features:\n",
    "    test_dataframe[feature] = 0\n",
    "\n",
    "# Reorder columns to match the training data\n",
    "test_dataframe = test_dataframe[X_train.columns]\n",
    "\n",
    "# Make predictions for the test instance\n",
    "prediction = model.predict(test_dataframe)[0]\n",
    "\n",
    "# Convert the prediction back to original label\n",
    "predicted_label = \"No\" if prediction == 1 else \"Yes\"\n",
    "\n",
    "print(\"\\nTest Instance:\")\n",
    "for feature, value in test_instance.items():\n",
    "    if value == 1:\n",
    "        print(\"\\t\\t\", f\"\\t{feature} = 1\")\n",
    "\n",
    "print(\"\\nPrediction:\", predicted_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Sir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "dataSet = {\n",
    "    \"CPU Usage\": [\"High\", \"High\", \"Low\", \"High\", \"Low\", \"Low\", \"High\", \"Low\", \"Low\", \"High\", \"Low\", \"High\", \"High\", \"Low\"],\n",
    "    \"Memory Usage\": [\"High\", \"Low\", \"Low\", \"High\", \"Low\", \"High\", \"Low\", \"High\", \"Low\", \"Low\", \"High\", \"Low\", \"Low\", \"High\"],\n",
    "    \"Disk Usage\": [\"High\", \"High\", \"Low\", \"High\", \"Low\", \"Low\", \"High\", \"High\", \"Low\", \"Low\", \"High\", \"High\", \"High\", \"Low\"],\n",
    "    \"Network Usage\": [\"Low\", \"High\", \"Low\", \"Low\", \"High\", \"High\", \"Low\", \"Low\", \"High\", \"Low\", \"High\", \"High\", \"Low\", \"High\"],\n",
    "    \"Is Slow\": [\"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\"]\n",
    "}\n",
    "\n",
    "dataFrame = pd.get_dummies(pd.DataFrame(dataSet))\n",
    "X = dataFrame.drop([\"Is Slow_No\", \"Is Slow_Yes\"], axis=1)\n",
    "y = dataFrame[\"Is Slow_Yes\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "predicted_labels = [\"No\" if pred == 0 else \"Yes\" for pred in y_pred]\n",
    "\n",
    "test_instance = {\n",
    "    \"CPU Usage_High\": 1,\n",
    "    \"Memory Usage_High\": 1,\n",
    "    \"Disk Usage_High\": 1,\n",
    "    \"Network Usage_Low\": 1\n",
    "}\n",
    "test_dataframe = pd.DataFrame([test_instance])\n",
    "missing_features = set(X_train.columns) - set(test_dataframe.columns)\n",
    "for feature in missing_features:\n",
    "    test_dataframe[feature] = 0\n",
    "test_dataframe = test_dataframe[X_train.columns]\n",
    "\n",
    "prediction = model.predict(test_dataframe)[0]\n",
    "predicted_label = \"No\" if prediction == 0 else \"Yes\"\n",
    "\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Sir 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "dataSetStudent = {\n",
    "    \"age\": [\"<=30\", \"<=30\", \"30...40\", \">40\", \">40\", \">40\", \"31...40\", \"<=30\", \"<=30\", \">40\", \"<=30\", \"31...40\", \"31...40\", \">40\"],\n",
    "    \"income\": [\"high\", \"high\", \"high\", \"medium\", \"low\", \"low\", \"low\", \"medium\", \"low\", \"medium\", \"medium\", \"medium\", \"high\", \"medium\"],\n",
    "    \"student\": [\"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\"],\n",
    "    \"credit_rating\": [\"fair\", \"excellent\", \"fair\", \"fair\", \"fair\", \"excellent\", \"excellent\", \"fair\", \"fair\", \"fair\", \"excellent\", \"excellent\", \"fair\", \"excellent\"],\n",
    "    \"buys_computer\": [\"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"]\n",
    "}\n",
    "\n",
    "dataFrame = pd.get_dummies(pd.DataFrame(dataSetStudent))\n",
    "features = dataFrame.drop([\"buys_computer_no\", \"buys_computer_yes\"], axis=1)\n",
    "y = dataFrame[\"buys_computer_no\"]\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(features, y)\n",
    "\n",
    "test_instance = {\n",
    "    \"age_<=30\": 1,\n",
    "    \"income_medium\": 1,\n",
    "    \"student_yes\": 1,\n",
    "    \"credit_rating_fair\": 1\n",
    "}\n",
    "\n",
    "test_dataframe = pd.DataFrame([test_instance])\n",
    "missing_features = set(features.columns) - set(test_dataframe.columns)\n",
    "for feature in missing_features:\n",
    "    test_dataframe[feature] = 0\n",
    "test_dataframe = test_dataframe[features.columns]\n",
    "\n",
    "predictions = model.predict(test_dataframe)\n",
    "predicted_labels = [\"no\" if pred == 1 else \"yes\" for pred in predictions]\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
